# Optimization_Methods_Coursera
This is the completed Lab Assignment from Coursera's "Improving Deep Neural Networks : Hyperparameter Tuning".
Since this was a Coursera Assignment made for beginners, this was mostly already coded (like a template) with few blanks meant for the students to fill out the missing algorithm.

## Optimization Methods Explored - 
- Gradient Descent (Batch & Gradient Desecent)
- Mini Batch Gradient Descent
- Gradient Descent with Momentum
- Adam Optimization
